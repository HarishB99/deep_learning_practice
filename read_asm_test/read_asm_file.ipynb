{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "asm_files_dir = './dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "asm_files = os.listdir(asm_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_data_seg(line):\n",
    "    match = re.search('^\\.[a-z]{0,1}data', line)\n",
    "    match_found = False\n",
    "    try:\n",
    "        found = match.group(0)\n",
    "        match_found = True\n",
    "    except AttributeError:\n",
    "        match_found = False\n",
    "    finally:\n",
    "        return match_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_text_seg(line):\n",
    "    match = re.search('^\\.text', line)\n",
    "    match_found = False\n",
    "    try:\n",
    "        found = match.group(0)\n",
    "        match_found = True\n",
    "    except AttributeError:\n",
    "        match_found = False\n",
    "    finally:\n",
    "        return match_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_2digit_hex(text):\n",
    "    match = re.match('^[0-9A-F]{2}$', text)\n",
    "    match_found = False\n",
    "    try:\n",
    "        found = match.group(0)\n",
    "        match_found = True\n",
    "    except AttributeError:\n",
    "        match_found = False\n",
    "    finally:\n",
    "        return match_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_text_comment(text):\n",
    "    match = re.match('^_[a-z]?text$', text)\n",
    "    match_found = False\n",
    "    try:\n",
    "        found = match.group(0)\n",
    "        match_found = True\n",
    "    except AttributeError:\n",
    "        match_found = False\n",
    "    finally:\n",
    "        return match_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_data_comment(text):\n",
    "    match = re.match('^_[a-z]?data$', text)\n",
    "    match_found = False\n",
    "    try:\n",
    "        found = match.group(0)\n",
    "        match_found = True\n",
    "    except AttributeError:\n",
    "        match_found = False\n",
    "    finally:\n",
    "        return match_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    with open(file, 'r', encoding='ISO-8859-1') as f:\n",
    "        struct_dict = {\n",
    "            \"text_arr\": [],\n",
    "            \"data_arr\": [],\n",
    "            \"file_name\": file\n",
    "        }\n",
    "        for asm_line in f:\n",
    "            asm_line = asm_line.strip()\n",
    "            if '.text' in asm_line:\n",
    "                struct_dict[\"text_arr\"].append(asm_line)\n",
    "            elif is_data_seg(asm_line):\n",
    "                struct_dict[\"data_arr\"].append(asm_line)\n",
    "            else:\n",
    "                continue\n",
    "        return struct_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_of_comment(arr):\n",
    "    indices = [ i for i, token in enumerate(arr) if token.startswith(';') ]\n",
    "    if len(indices) > 0:\n",
    "        return indices[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_commas(line_arr):\n",
    "    newlinearr = []\n",
    "    for line in line_arr:\n",
    "        newline = []\n",
    "        for token in line:\n",
    "            if ',' in token:\n",
    "                temp = token.split(',')\n",
    "                temp = [ item for item in temp if item != '' ]\n",
    "                for item in temp:\n",
    "                    newline.append(item)\n",
    "            else:\n",
    "                newline.append(token)\n",
    "        newlinearr.append(newline)\n",
    "    return newlinearr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_text(text_arr):\n",
    "    # Split by whitespace each line in text_arr\n",
    "    text_arr = [ line.split() for line in text_arr ]\n",
    "    # Remove all comments from each line (array)\n",
    "    text_arr = [ line[:(start_of_comment(line))] for line in text_arr ]\n",
    "    # Remove the first word (\".text*\") \n",
    "    # from each line (array).\n",
    "    text_arr = [ [token for token in line if not is_text_seg(token)] for line in text_arr ]\n",
    "    # Remove hexadecimal numbers (purpose is to \n",
    "    # remove the first few hex numbers which probably \n",
    "    # is the hex representation of the opcodes)\n",
    "    text_arr = [ [token for token in line if not is_2digit_hex(token)] for line in text_arr ]\n",
    "    # Remove all '??' from line\n",
    "    text_arr = [ [token for token in line if token != '??'] for line in text_arr ]\n",
    "    # Split all tokens using ','\n",
    "    text_arr = remove_commas(text_arr)\n",
    "    # Remove all empty line (array).\n",
    "    text_arr = [ line for line in text_arr if line != [] ]\n",
    "    return text_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_data(data_arr):\n",
    "    # Split by whitespace each line in data_arr\n",
    "    data_arr = [ line.split() for line in data_arr ]\n",
    "    # Remove all comments from each line (array)\n",
    "    data_arr = [ line[:(start_of_comment(line))] for line in data_arr ]\n",
    "    # Remove the first word (\".*data*\") \n",
    "    # from each line (array).\n",
    "    data_arr = [ [token for token in line if not is_data_seg(token)] for line in data_arr ]\n",
    "    # Remove hexadecimal numbers (purpose is to \n",
    "    # remove the first few hex numbers which probably \n",
    "    # is the hex representation of the opcodes)\n",
    "    data_arr = [ [token for token in line if not is_2digit_hex(token)] for line in data_arr ]\n",
    "    # Remove all '??' from line\n",
    "    data_arr = [ [token for token in line if token != '??'] for line in data_arr ]\n",
    "    # Split all tokens using ','\n",
    "    data_arr = remove_commas(data_arr)\n",
    "    # Remove all empty line (array).\n",
    "    data_arr = [ line for line in data_arr if line != [] ]\n",
    "    return data_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(line_arr, keywords_dict):\n",
    "    processed_line_arr = [ [keywords_dict.get(token, token) for token in line] for line in line_arr ]  # In production, change 'keywords_dict.get(token, token)' to 'keywords_dict.get(token, None)' and remove all 'false'y values thereafter\n",
    "    processed_line_arr = [ [token for token in line if token] for line in processed_line_arr ]\n",
    "    return processed_line_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_processing(line_arr):  # If approved, then add this to process_dataset\n",
    "#     processed_line_arr1 = []\n",
    "#     for i, line in enumerate(line_arr):\n",
    "#         if len(line) < 1:\n",
    "#             print(i, ': ', line)\n",
    "#         elif not (is_text_comment(line[0]) or is_data_comment(line[0])):\n",
    "#             processed_line_arr1.append(line)\n",
    "    \n",
    "#     processed_line_arr2 = []\n",
    "#     for i, line in enumerate(processed_line_arr1):\n",
    "#         if len(line) < 1:\n",
    "#             print(i, ': ', line)\n",
    "#         elif not (line[0].startswith('assume')):\n",
    "#             processed_line_arr2.append(line)\n",
    "    \n",
    "#     processed_line_arr = []\n",
    "#     for i, line in enumerate(processed_line_arr2):\n",
    "#         if len(line) < 1:\n",
    "#             print(i, ': ', line)\n",
    "#         elif not (line[0].endswith(':')):\n",
    "#             processed_line_arr.append(line)\n",
    "    processed_line_arr = [ line for line in line_arr if (not (is_text_comment(line[0]) or is_data_comment(line[0]))) ]\n",
    "    processed_line_arr = [ line for line in processed_line_arr if (not (line[0].startswith('assume'))) ]\n",
    "    processed_line_arr = [ line for line in processed_line_arr if (not (line[0].endswith(':'))) ]\n",
    "    return processed_line_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arr = [struct_dict for struct_dict in map(lambda x: read_file(os.path.join(asm_files_dir, x)), asm_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arr_2 = [{ \n",
    "    \"text_arr\": cleanse_text(struct_dict[\"text_arr\"]), \n",
    "    \"data_arr\": cleanse_data(struct_dict[\"data_arr\"]), \n",
    "    \"file_name\": struct_dict[\"file_name\"] \n",
    "} for struct_dict in dataset_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./keywordsdict.txt', 'r') as f:\n",
    "    keywords_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arr_3 = [{ \n",
    "    \"text_arr\": process_dataset(struct_dict[\"text_arr\"], keywords_dict), \n",
    "    \"data_arr\": process_dataset(struct_dict[\"data_arr\"], keywords_dict), \n",
    "    \"file_name\": struct_dict[\"file_name\"] \n",
    "} for struct_dict in dataset_arr_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./dataset/02IOCvYEy8mjiuAQHax3.asm'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_arr_3[0][\"file_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arr_4 = [{ \n",
    "    \"text_arr\": extra_processing(struct_dict[\"text_arr\"]), \n",
    "    \"data_arr\": extra_processing(struct_dict[\"data_arr\"]), \n",
    "    \"file_name\": struct_dict[\"file_name\"] \n",
    "} for struct_dict in dataset_arr_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
